PropBot logic and guidelines         
You are an expert in web scraping and data extraction, with a focus on Python libraries and frameworks such as requests, BeautifulSoup, selenium, and advanced tools like jina, firecrawl, agentQL, and multion.

        Key Principles:
        - Write concise, technical responses with accurate Python examples.
        - Prioritize readability, efficiency, and maintainability in scraping workflows.
        - Use modular and reusable functions to handle common scraping tasks.
        - Handle dynamic and complex websites using appropriate tools (e.g., Selenium, agentQL).
        - Follow PEP 8 style guidelines for Python code.

        General Web Scraping:
        - Use requests for simple HTTP GET/POST requests to static websites.
        - Parse HTML content with BeautifulSoup for efficient data extraction.
        - Handle JavaScript-heavy websites with selenium or headless browsers.
        - Respect website terms of service and use proper request headers (e.g., User-Agent).
        - Implement rate limiting and random delays to avoid triggering anti-bot measures.

        Text Data Gathering:
        - Use jina or firecrawl for efficient, large-scale text data extraction.
            - Jina: Best for structured and semi-structured data, utilizing AI-driven pipelines.
            - Firecrawl: Preferred for crawling deep web content or when data depth is critical.
        - Use jina when text data requires AI-driven structuring or categorization.
        - Apply firecrawl for tasks that demand precise and hierarchical exploration.

        Handling Complex Processes:
        - Use agentQL for known, complex processes (e.g., logging in, form submissions).
            - Define clear workflows for steps, ensuring error handling and retries.
            - Automate CAPTCHA solving using third-party services when applicable.
        - Leverage multion for unknown or exploratory tasks.
            - Examples: Finding the cheapest plane ticket, purchasing newly announced concert tickets.
            - Design adaptable, context-aware workflows for unpredictable scenarios.

        Data Validation and Storage:
        - Validate scraped data formats and types before processing.
        - Handle missing data by flagging or imputing as required.
        - Store extracted data in appropriate formats (e.g., CSV, JSON, or databases such as SQLite).
        - For large-scale scraping, use batch processing and cloud storage solutions.
        - Track neighborhood average prices and maintain historical price trends.
        - Include first_seen_date for all property listings to track market duration.

        Error Handling and Retry Logic:
        - Implement robust error handling for common issues:
            - Connection timeouts (requests.Timeout).
            - Parsing errors (BeautifulSoup.FeatureNotFound).
            - Dynamic content issues (Selenium element not found).
        - Retry failed requests with exponential backoff to prevent overloading servers.
        - Log errors and maintain detailed error messages for debugging.

        Performance Optimization:
        - Optimize data parsing by targeting specific HTML elements (e.g., id, class, or XPath).
        - Use asyncio or concurrent.futures for concurrent scraping.
        - Implement caching for repeated requests using libraries like requests-cache.
        - Profile and optimize code using tools like cProfile or line_profiler.

        Dependencies:
        - requests
        - BeautifulSoup (bs4)
        - selenium
        - jina
        - firecrawl
        - agentQL
        - multion
        - lxml (for fast HTML/XML parsing)
        - pandas (for data manipulation and cleaning)

        Key Conventions:
        1. Begin scraping with exploratory analysis to identify patterns and structures in target data.
        2. Modularize scraping logic into clear and reusable functions.
        3. Document all assumptions, workflows, and methodologies.
        4. Use version control (e.g., git) for tracking changes in scripts and workflows.
        5. Follow ethical web scraping practices, including adhering to robots.txt and rate limiting.
        Refer to the official documentation of jina, firecrawl, agentQL, and multion for up-to-date APIs and best practices.

Below are general instructions to maintain best practices and code standards, while keeping the project directory clean and avoiding unnecessary proliferation of functions and code versions:
1. Code Quality & Consistency
Follow Established Coding Standards:


Adhere to PEP8 for Python, and relevant style guides for JavaScript.


Use consistent naming conventions, indentation, and file organization.


Refactor, Don't Recreate:


Modify and improve existing functions rather than adding redundant ones.


Avoid introducing new functions unless they address a clearly identified gap and can be reused across the project.


Code Reviews & Testing:


Implement regular code reviews to ensure quality and adherence to standards.


Write unit and integration tests for any changes to prevent regressions.


2. Project Directory & File Organization
Maintain a Clean Directory Structure:


Keep all modules organized in clearly defined packages (e.g., scrapers, analysis, reporting, UI, utils).


Avoid scattering functions across multiple files; group related functions together in the same module.


Version Control Best Practices:


Use Git (or a similar version control system) to manage code changes.


Commit changes with clear, descriptive messages; avoid committing untested or experimental code branches.


Documentation & Comments:


Update inline comments and external documentation whenever modifications are made.


Ensure that each file has a clear header describing its purpose and module responsibilities.


3. Minimalistic Editing Approach
Edit Existing Files:


Rather than creating new versions of files, update the current files to improve functionality or fix issues.


This approach minimizes code duplication and maintains a single source of truth for each module.


Prevent Feature Creep:


Introduce new functionality only after careful consideration and when it aligns with the overall project goals.


Ensure that any changes are fully integrated and tested within the existing framework before moving on.


4. Continuous Improvement & Monitoring
Regularly Audit the Codebase:


Schedule periodic reviews of the project structure and code quality.


Remove any deprecated functions or redundant code segments to keep the project lean.


Automate Where Possible:


Use linting tools, unit testing frameworks, and continuous integration to maintain code quality automatically.


Integrate automated scripts to manage cleanup tasks, ensuring that the processed datasets and code directories remain uncluttered.


investmentbot/
├── propbot/             # PropBot application
│   ├── analysis/        # Analysis modules
│   │   ├── expenses/    # Expense and tax calculation
│   │   └── metrics/     # Investment metrics and rental analysis
│   ├── data/            # Data management
│   │   ├── collection/  # Web scraping and data collection
│   │   ├── processing/  # Data cleaning and transformation
│   │   ├── raw/         # Raw data storage
│   │   └── processed/   # Processed data and reports
│   ├── ui/              # User interface components
│   │   ├── static/      # Static assets (CSS, JS)
│   │   └── templates/   # HTML templates
│   └── utils/           # Utility functions and helpers
├── config/              # Configuration files
├── docs/                # Documentation
└── [other directories]  # Other project components


        
Botlogic and workflow:
1. Data Collection Module
Scraping Functions:


idealista_scraper.py & rental_scraper.py:


Fetch sales and rental listings using the ScrapingBee API.


Use premium proxies and JavaScript rendering as needed to bypass anti-bot measures.


scrape_additional_rentals.py:


Construct and iterate through paginated URLs for additional rental data.


Scheduler (scheduler.py / monthly_rental_scheduler.py):


Schedule the scraping tasks (daily for sales data, monthly for rental data).


2. Data Normalization and Location Matching
Location Standardization: Data Normalization and Location Matching
Location Standardization
Scripts: improve_location_matching.py & validate_location_matching.py
Standardize location names using mapping dictionaries and fuzzy matching techniques.
Validate location consistency and generate comprehensive reports.
Maintain an updated list of Lisbon neighborhoods:
Alfama, Baixa, Chiado, Bairro Alto, Belém, Cais do Sodré, Graça, Lapa, Príncipe Real, Avenida da Liberdade, Avenidas Novas, Campo de Ourique, Estrela, Marquês de Pombal, Parque das Nações, Saldanha, Alvalade, Anjos, Intendente, Arroios, Alameda, Areeiro, Roma, Martim Moniz, Rossio, São Vicente, Santa Clara, Marvila, Olivais, Benfica, Carnide, Lumiar, Ajuda, Alcântara, Campolide, Beato, Penha de França, São Domingos de Benfica
Rental Analysis Enhancements
Scripts: rental_analysis.py / updated.py
Data Extraction
Key Attributes: Extract essential listing details: property size, room type, neighborhood, and rent price.

Rental Analysis Enhancements:


rental_analysis.py / updated.py:


Data Extraction:


Key Attributes: We extract essential information from each rental listing, including property size, room type, neighborhood, and the current rent price.


Identification of Comparables:


Filtering: Using these attributes, we apply functions to compare properties and select those that closely match the target listing.


Matching Criteria: The process involves fuzzy matching on the neighborhood and checking that size and room type fall within acceptable thresholds.


Average Rent Calculation:


Aggregation: Once the set of comparables is identified, the function calculates the average rent from these listings.


Estimation: This average provides the expected rent estimate, ensuring that the calculation reflects the current market dynamics.

3. Expense and Tax Analysis
Expense Calculation: step-by-step breakdown of the expense calculation process:
Logging


The function begins by logging each key action with a timestamp. This helps trace the calculation flow and debug any issues that arise.


Retrieving Default Expense Parameters


A set of default parameters (e.g., property management fees, maintenance costs, vacancy loss, insurance, and utilities) is retrieved.


These defaults serve as the baseline expense values for a property investment.


User Confirmation and Adjustments


Before proceeding, the function allows the user to review and adjust these default expense parameters, ensuring that any property-specific or market-specific nuances are considered.


Tax Calculation


Determine the Tax Bracket:


The property's value is used to determine the appropriate tax bracket from a predefined simplified tax table.


For example, a property valued at 250,000 € falls into the bracket "mais de 194.458,00 € até 324.058,00 €," which applies a 7% rate and a deduction of 9.210,31 €.


Calculate the IMT (Property Transfer Tax):


Multiply the property value by the tax rate and subtract the corresponding deduction:


Calculation: (250,000 € x 7%) - 9.210,31 € = 17,500 € - 9.210,31 € = 8.289,69 €


Calculate the Stamp Duty (Imposto de Selo):


Apply a flat rate of 0.8% on the property value:


Calculation: 250,000 € x 0.8% = 2,000 €


Total Tax Payable:


Sum the IMT and Stamp Duty to obtain the total property tax:


Total: 8.289,69 € + 2,000 € = 10.289,69 €


Calculating Recurring Expenses


The function computes ongoing expenses over time (such as property management, maintenance, vacancy, insurance, and utilities) using the confirmed expense parameters.


These recurring expenses are critical to determine the overall cost of owning and managing the property.


Expense Report Generation


All calculated expenses (both recurring and one-time, such as taxes) are aggregated into a comprehensive expense report.


The report is formatted for ease of analysis and can be saved in various formats (e.g., JSON, CSV) for further review or record-keeping.


4. Investment Metrics Calculation
Metric Functions:


calculate_rental_income: Estimate monthly rental income based on property size and comparable rent.


calculate_operating_expenses: Calculate total operating expenses including:


- Property management fees


- Maintenance costs


- Vacancy loss


- Insurance


- Utilities


calculate_noi: Subtract operating expenses from annual rent to get NOI.


calculate_cap_rate: Divide NOI by the purchase price.


calculate_cash_on_cash_return: Compute return on total investment (purchase price plus one-time costs).


calculate_monthly_cash_flow: Determine monthly net cash flow after all expenses.


calculate_neighborhood_metrics: Calculate and track:


- Average price per sqm per neighborhood


- Historical price trends by neighborhood


- Price deviation from neighborhood average


- Market duration statistics


Segmentation Function:


segment_properties_by_area:


- Group properties by standardized neighborhood


- Calculate average price per sqm per neighborhood


- Compare each property's price per sqm relative to the neighborhood average


- Label properties (below average, average, above average)


- Track historical price trends and market duration


5. Investment Summary and Reporting
Summary Generation:


create_investment_summary:


For each property, compile key metrics (purchase price, monthly/annual rent, NOI, Cap Rate, Cash on Cash Return, monthly cash flow, price per sqm, and segmentation details).


present_investment_summary:


Format the summary as a comparative table (HTML/CSV) for easy analysis.


6. User Interface and Filtering
Frontend Module (investment_filter.js):


- Provide interactive table controls to filter, sort, and highlight:


- Top deals based on investment metrics


- Properties below neighborhood average price


- New listings and price changes


- Market duration indicators


- Update row counts and display real-time summaries


- Enable sorting by neighborhood average price comparison


- Highlight price deviations from neighborhood averages


- Track and display historical price trends


Dashboard Features:


- Investment metrics table with sortable columns


- Neighborhood average price per sqm comparison


- Market duration tracking with first_seen_date


- Historical price trend visualization


- Property price deviation indicators


- Interactive filtering and sorting capabilities


7. Overall Integration Workflow
Outline the data flow and processing pipeline for the PropBot real estate analysis system, a component of the investmentbot project.

Key Data Types:

Sales Data: Property listings for sale.

Rental Data: Property listings for rent.

General Workflow:

mathematica
Copy
Raw Data → Consolidation → Standardized CSV → Analysis → Reports
2. Data Structure & Sources
2.1 Sales Data
Primary Source:

idealista_listings.json (project root)

Consolidated Source:

investmentbot/propbot/data/processed/sales_listings_consolidated.json

Standardized CSV:

investmentbot/propbot/data/processed/sales.csv

2.2 Rental Data
Primary Sources:

investmentbot/propbot/data/raw/rentals/rental_listings.json (from scraping)

rental_complete.csv (legacy dataset, project root)

Consolidated Source:

investmentbot/propbot/data/processed/rental_listings_consolidated.json

Standardized CSV:

investmentbot/propbot/data/processed/rentals.csv

3. Data Processing Workflow
3.1 Data Consolidation
Sales Consolidation:

Command: python3 -m investmentbot.propbot.consolidate_sales

Merges data from idealista_listings.json and other sales files to create sales_listings_consolidated.json.

Rental Consolidation:

Command: python3 -m investmentbot.propbot.consolidate_rentals

Merges data from rental_listings.json and rental_complete.csv to create rental_listings_consolidated.json.

3.2 Data Standardization
Sales Standardization:

Command: python3 -m investmentbot.propbot.convert_sales_from_consolidated

Converts consolidated sales JSON into sales.csv with standardized fields (price, size, room type, etc.).

Rental Standardization:

Command: python3 -m investmentbot.propbot.convert_from_consolidated

Converts consolidated rental JSON into rentals.csv with standardized rental data fields.

3.3 Analysis
Rental Income Analysis:

Command: python3 -m investmentbot.propbot.run_rental_analysis

Processes both sales.csv and rentals.csv to generate rental income reports in JSON and CSV formats.

4. Data Cleaning & Normalization
4.1 Setup & Configuration
Logging and Directory Structure:

Initializes logging and sets up directories (BASE_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR).

Creates necessary directories and defines file paths.

4.2 Data Extraction Functions
Key Functions:

extract_size(): Parses property sizes (e.g., "70 m²", "T270 m²").

extract_room_type(): Extracts room types (T0-T4).

extract_price(): Converts price strings to numbers.

extract_location(): Cleans and extracts location data.

4.3 Loading Raw Data
Function:

load_raw_json(): Loads property data from JSON files.

4.4 Processing Listings
Functions:

process_rental_listings(): Transforms raw rental data to standardized format.

process_sales_listings(): Transforms raw sales data similarly.

Both functions extract essential attributes (price, size, location, etc.).

4.5 Saving Processed Data
Function:

save_to_csv(): Saves processed data to CSV files.

Creates backups before overwriting.

4.6 Metadata Management
Function:

update_metadata(): Tracks processing history, including property counts and update timestamps.

4.7 Main Processing Pipeline
Function:

process_data(): Orchestrates the complete workflow—loading, processing, saving, and metadata updates.

Execution:

Automatically runs as a script when executed.

5. Typical Workflow After Data Scraping
Update Consolidated Files:

bash
Copy
python3 -m investmentbot.propbot.consolidate_sales
python3 -m investmentbot.propbot.consolidate_rentals
Convert to Standardized Format:

bash
Copy
python3 -m investmentbot.propbot.convert_sales_from_consolidated
python3 -m investmentbot.propbot.convert_from_consolidated
Run Analysis:

bash
Copy
python3 -m investmentbot.propbot.run_rental_analysis
6. User Interface
6.1 Overall Design Principles
Minimalistic Layout:

Ample whitespace, grid-based structure, and a focus on key data.

Clear Typography:

Use clean, sans-serif fonts with consistent sizes and weights.

Straightforward Navigation:

Fixed navigation bar with essential sections (Dashboard, Neighborhood, Price, Size, etc.).

6.2 Layout & Content Organization
Dashboard Overview:

Header: Application name/logo and a tagline.

Navigation Bar: Quick links to key sections such as Neighborhood, Price, Size, etc.

Main Content Area:

Data Display: Tables or cards showing key metrics:

Neighborhood, Price, Size, Number of Rooms, Estimated Rent, NOI, Cap Rate, Gross Yield, Cash on Cash Return, Monthly Cash Flow, Price per sqm.

Interactive Elements:

Filters, sort options, responsive search, and hover details for data insights.

6.3 Visual Aids & Reporting
Charts and Graphs:

Simple, modern charts with clear legends and tooltips to visualize trends.

6.4 Color Palette & Branding
Professional & Clean Colors:

Neutral tones (grays, whites) with blue accents for highlights.

Consistency:

Uniform use of colors across buttons, links, and interactive elements.

Color coding for data (e.g., positive vs. negative cash flows).


Data Cleaning and Normalization:


Setup & Configuration
Initializes logging system
Sets up directory structure (BASE_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR)
Creates necessary directories if they don't exist
Defines file paths for input and output files
Data Extraction Functions
extract_size(): Parses property size from strings like "70 m²" or "T270 m²"
extract_room_type(): Extracts room types (T0-T4) from property details
extract_price(): Converts price strings to numerical values
extract_location(): Cleans and extracts location information
Loading Raw Data
load_raw_json(): Loads property data from JSON files
Processing Listings
process_rental_listings(): Transforms raw rental data into standardized format
process_sales_listings(): Transforms raw sales data into standardized format
Both extract essential property attributes (price, size, location, etc.)
Saving Processed Data
save_to_csv(): Saves processed data to CSV files
Creates backups of existing files before overwriting
Metadata Management
update_metadata(): Maintains a record of processing history
Tracks counts of properties and timestamps of updates
Main Processing Pipeline
process_data(): Orchestrates the entire workflow
Loads raw data → Processes listings → Saves to CSV → Updates metadata
Execution
When run as a script, automatically executes the processing pipeline
The script handles various data cleaning challenges including:
Special formatting patterns (T2 room types with attached sizes)
Inconsistent price formats (with commas, dots, thousands separators)
Missing or malformed data
Different data structures in input files





User Interface:
Overall Design Principles
Minimalistic Layout:

Emphasize ample whitespace to reduce clutter and direct the user's focus on key data.

Use a simple grid-based structure that organizes content logically.

Clear, Readable Typography:

Select clean, sans-serif fonts (e.g., Helvetica, Roboto) for easy reading.

Use consistent font sizes and weights to differentiate headings, labels, and body text.

Straightforward Navigation:

Incorporate a clear, fixed navigation bar with essential sections (Dashboard, Neighborhood, Price, Size, Number of Rooms, Estimated Rent, and all financial metrics).

Use intuitive icons paired with text labels to enhance usability.

Layout & Content Organization
Dashboard Overview:

Header: Include the application name/logo and a brief tagline emphasizing professionalism and data-driven insights.

Navigation Bar: Provide quick links to:

Neighborhood

Price

Size

Number of Rooms

Estimated Rent

Financial Metrics

Main Content Area:

Data Table or Cards:

Display key property metrics in a clean, well-spaced table or card format.

Columns/Sections should include:

Neighborhood

Price

Size

Number of Rooms

Estimated Rent

NOI (€)

Cap Rate (%)

Gross Yield (%)

Cash on Cash Return (%)

Monthly Cash Flow (€)

Price per sqm (€)

Interactive Elements:

Include filter and sort options that are intuitive and require minimal clicks.

Provide responsive search and hover details for quick data insights.

Visual Aids & Reporting:

Use simple, modern charts or graphs to visualize trends in key metrics.

Ensure charts have clear legends, tooltips, and are color-coordinated with the overall design palette.

Color Palette & Branding
Professional & Clean Colors:

Use a neutral color palette (e.g., shades of gray, white, and blue accents) to evoke a sense of trust and professionalism.

Ensure that accent colors are used sparingly to highlight critical data points or call-to-action buttons.

Consistency:

Maintain consistency in colors across buttons, links, and interactive elements.

Use color coding where necessary (e.g., positive vs. negative cash flows) for quick visual analysis.



Scheduling and Automation:


Set up recurring tasks for data scraping and analysis (daily for sales, monthly for rental data) to keep the app up-to-date.



Error Handling Strategy
1. Data Collection Error Handling
API Failure Recovery:
Implement exponential backoff retry logic in idealista_scraper.py and rental_scraper.py
Log failed requests with timestamps and error codes
Automatically resume scraping from the last successful page when restarted
Credit Limit Management:
Monitor ScrapingBee credit usage via load_credits_usage() and update_credits_usage()
Implement an early termination mechanism when approaching credit limits
Cache partial results to avoid data loss when stopping mid-scrape
Data Validation During Collection:
Verify minimum required fields are present in scraped listings
Flag and log malformed or suspicious data patterns
Create a separate queue for problematic listings that require manual review
2. Processing Error Handling
Robust Data Extraction:
Enhance extract_size(), extract_room_type(), and extract_neighborhood() with fallback patterns
Implement confidence scoring for extracted values
Flag properties with low-confidence extractions for manual verification
Location Matching Fallbacks:
Create a hierarchical matching system in improve_location_matching.py
If neighborhood-level matching fails, fall back to district or city-level matching
Maintain an "unmatched locations" report for periodic review and mapping table updates
Calculation Safety:
Add division-by-zero and null value protections in all metric calculations
Use reasonable default values when data is missing but processing should continue
Generate "calculation warning" flags in the final report for properties with estimated values
3. Centralized Error Management
Error Logging Service:
Create a centralized error_logger.py module with different severity levels
Include context data with each error for easier debugging
Aggregate similar errors to identify systematic issues
Error Notification System:
Implement email/Slack alerts for critical failures
Create a daily error digest for non-critical issues
Set up different notification thresholds based on error frequency and impact
Recovery Procedures:
Document automated and manual recovery procedures for common failure modes
Implement a "safe mode" that can run with reduced functionality when certain components fail
Incremental Update Strategy
1. Property Tracking System
Unique Identifier Management:
Extract and maintain consistent property IDs from URLs or listing data
Create a persistent property registry to track all previously seen properties
Implement a hashing mechanism for listings without explicit IDs
Change Detection:
Compare new listings against existing data using property IDs
Check for changes in critical fields (price, size, description)
Flag significant changes (e.g., price drops >5%) for highlighting in reports
2. Differential Scraping
Targeted Collection:
Modify scraper to prioritize new listings and recently updated properties
Implement "light" and "deep" scraping modes (metadata only vs. full details)
Create a scheduling algorithm that adjusts scraping frequency based on market activity
Partial Data Updates:
Update only changed fields rather than replacing entire property records
Implement a database schema or file structure that supports field-level updates
Maintain atomic update operations to prevent data corruption during updates
3. Historical Data Management
Version Control for Listings:
Track the history of changes for each property (especially price changes)
Implement a time-series storage approach for key metrics
Create functions to analyze property history (time on market, price adjustments)
Update Audit Trail:
Log all data modifications with timestamps and change sources
Create a mechanism to roll back problematic updates
Generate reports on data freshness and update coverage
4. Optimized Scheduling
Adaptive Scheduling:
Modify scheduler.py to adjust scraping frequency based on:
Day of week (weekend vs. weekday listing patterns)
Time since last full update
Market activity levels
Partial Update Cycles:
Implement different update cycles for different data types:
Daily: New listings and price changes
Weekly: Full details for active listings
Monthly: Comprehensive rental data update
Quarterly: Neighborhood statistics recalculation
Update Coordination:
Ensure dependent data (sales listings, rental comparables) are updated in logical sequence
Implement "update markers" to trigger downstream processes when prerequisite data changes
Optimize update windows to minimize processing during high-traffic periods
This enhanced plan addresses both the error handling and incremental update strategies while maintaining the overall structure and flow of the original integration plan.

